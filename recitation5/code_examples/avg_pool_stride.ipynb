{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ops = edf.ops\n",
    "params = edf.params\n",
    "values = edf.values\n",
    "\n",
    "# Average pooling with stride\n",
    "class avg_pool_with_stride:\n",
    "    def __init__(self,x,sz,stride):\n",
    "        ops.append(self)\n",
    "        self.x = x\n",
    "        self.sz = sz\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self):\n",
    "        B,H,W,C = self.x.top.shape\n",
    "        nH, nW = (H-self.sz)//self.stride+1, (W-self.sz)//self.stride+1\n",
    "        top = np.zeros([B,nH,nW,C])\n",
    "        for i in range(self.sz):\n",
    "            for j in range(self.sz):\n",
    "                xcrop = self.x.top[:, i:(H-self.sz+1+i):self.stride, j:(W-self.sz+1+j):self.stride, :].copy()\n",
    "                top = top + xcrop\n",
    "        \n",
    "        self.top = top / np.float32(self.sz*self.sz)\n",
    "#         print(self.x.top[0,:,:,0])\n",
    "#         print(self.top[0,:,:,0])\n",
    "\n",
    "    def backward(self):\n",
    "        if self.x in ops or self.x in params:\n",
    "            B,H,W,C = self.x.top.shape\n",
    "            xgrad = np.zeros([B,H,W,C])\n",
    "            for i in range(self.sz):\n",
    "                for j in range(self.sz):\n",
    "                    xgrad[:, i:(H-self.sz+1+i):self.stride, j:(W-self.sz+1+j):self.stride, :] += self.grad / (self.sz*self.sz)\n",
    "\n",
    "            self.x.grad = self.x.grad + xgrad\n",
    "#             print(self.grad[0,:,:,0])\n",
    "#             print(self.x.grad[0,:,:,0])\n",
    "            \n",
    "            \n",
    "edf.avg_pool_with_stride = avg_pool_with_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################################\n",
    "\n",
    "# # Inputs and parameters\n",
    "# inp = edf.Param()\n",
    "# lab = edf.Value()\n",
    "\n",
    "# # Model\n",
    "# y = edf.avg_pool_with_stride(inp,2,2)\n",
    "\n",
    "# # Loss\n",
    "# loss = edf.add(y,lab)\n",
    "\n",
    "# # Forward test\n",
    "# data = np.arange(32).reshape([1,4,4,2])\n",
    "# inp.set(data)\n",
    "# l = np.ones([1,2,2,2])*(-1.0)\n",
    "# lab.set(l)\n",
    "\n",
    "# edf.Forward()\n",
    "\n",
    "# # Backward test\n",
    "# edf.Backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000: #### 0 Epochs: Val Loss = 2.324e+00, Accuracy = 9.60%\n",
      "000000050: Training Loss = 2.311e+00, Accuracy = 10.28%\n",
      "000000100: Training Loss = 2.290e+00, Accuracy = 12.32%\n",
      "000000150: Training Loss = 2.273e+00, Accuracy = 16.68%\n",
      "000000200: Training Loss = 2.250e+00, Accuracy = 16.32%\n",
      "000000250: Training Loss = 2.218e+00, Accuracy = 25.24%\n",
      "000000300: Training Loss = 2.170e+00, Accuracy = 41.12%\n",
      "000000350: Training Loss = 2.088e+00, Accuracy = 47.16%\n",
      "000000400: Training Loss = 1.950e+00, Accuracy = 54.60%\n",
      "000000450: Training Loss = 1.706e+00, Accuracy = 65.20%\n",
      "000000500: Training Loss = 1.320e+00, Accuracy = 72.32%\n",
      "000000500: #### 1 Epochs: Val Loss = 1.104e+00, Accuracy = 75.80%\n",
      "000000550: Training Loss = 1.012e+00, Accuracy = 75.00%\n",
      "000000600: Training Loss = 8.007e-01, Accuracy = 79.24%\n",
      "000000650: Training Loss = 6.916e-01, Accuracy = 80.28%\n",
      "000000700: Training Loss = 5.819e-01, Accuracy = 82.36%\n",
      "000000750: Training Loss = 5.467e-01, Accuracy = 84.16%\n",
      "000000800: Training Loss = 5.198e-01, Accuracy = 84.36%\n",
      "000000850: Training Loss = 5.401e-01, Accuracy = 84.32%\n",
      "000000900: Training Loss = 4.881e-01, Accuracy = 85.36%\n",
      "000000950: Training Loss = 4.926e-01, Accuracy = 84.72%\n",
      "000001000: Training Loss = 4.459e-01, Accuracy = 87.00%\n",
      "000001000: #### 2 Epochs: Val Loss = 4.233e-01, Accuracy = 87.30%\n",
      "000001050: Training Loss = 4.363e-01, Accuracy = 87.60%\n",
      "000001100: Training Loss = 4.477e-01, Accuracy = 86.60%\n",
      "000001150: Training Loss = 4.485e-01, Accuracy = 86.04%\n",
      "000001200: Training Loss = 4.652e-01, Accuracy = 85.24%\n",
      "000001250: Training Loss = 4.475e-01, Accuracy = 85.16%\n",
      "000001300: Training Loss = 4.745e-01, Accuracy = 84.76%\n",
      "000001350: Training Loss = 4.514e-01, Accuracy = 85.84%\n",
      "000001400: Training Loss = 4.284e-01, Accuracy = 87.40%\n",
      "000001450: Training Loss = 4.152e-01, Accuracy = 87.12%\n",
      "000001500: Training Loss = 4.139e-01, Accuracy = 87.44%\n",
      "000001500: #### 3 Epochs: Val Loss = 3.676e-01, Accuracy = 89.20%\n",
      "000001550: Training Loss = 4.009e-01, Accuracy = 87.96%\n",
      "000001600: Training Loss = 4.175e-01, Accuracy = 86.64%\n",
      "000001650: Training Loss = 4.136e-01, Accuracy = 87.28%\n",
      "000001700: Training Loss = 4.298e-01, Accuracy = 86.84%\n",
      "000001750: Training Loss = 4.083e-01, Accuracy = 88.40%\n",
      "000001800: Training Loss = 3.838e-01, Accuracy = 88.32%\n",
      "000001850: Training Loss = 3.939e-01, Accuracy = 87.60%\n",
      "000001900: Training Loss = 4.193e-01, Accuracy = 87.12%\n",
      "000001950: Training Loss = 4.085e-01, Accuracy = 87.44%\n",
      "000002000: Training Loss = 4.105e-01, Accuracy = 87.92%\n",
      "000002000: #### 4 Epochs: Val Loss = 3.586e-01, Accuracy = 89.40%\n",
      "000002050: Training Loss = 3.291e-01, Accuracy = 89.88%\n",
      "000002100: Training Loss = 3.984e-01, Accuracy = 88.00%\n",
      "000002150: Training Loss = 4.088e-01, Accuracy = 87.12%\n",
      "000002200: Training Loss = 3.889e-01, Accuracy = 87.68%\n",
      "000002250: Training Loss = 3.864e-01, Accuracy = 88.28%\n",
      "000002300: Training Loss = 4.188e-01, Accuracy = 87.20%\n",
      "000002350: Training Loss = 3.919e-01, Accuracy = 88.44%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-00d8db3ef4ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_lb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBSZ\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_acc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mniter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mniter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Courses/559TA/recitations/recitation5/code_examples/edf.py\u001b[0m in \u001b[0;36mForward\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Global forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Global backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Courses/559TA/recitations/recitation5/code_examples/edf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mstrided_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrided_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrided_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# shape: B x outH x outW x K^2 x C1 x C2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "from os.path import normpath as fn\n",
    "from time import time\n",
    "\n",
    "# Load data\n",
    "data = np.load(fn('inputs/mnist_26k.npz'))\n",
    "\n",
    "train_im = np.float32(data['im_train'])/255.-0.5\n",
    "train_im = np.reshape(train_im,[-1,28,28,1])\n",
    "train_lb = data['lbl_train']\n",
    "\n",
    "val_im = np.float32(data['im_val'])/255.-0.5\n",
    "val_im = np.reshape(val_im,[-1,28,28,1])\n",
    "val_lb = data['lbl_val']\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "# Inputs and parameters\n",
    "inp = edf.Value()\n",
    "lab = edf.Value()\n",
    "\n",
    "K1 = edf.Param()\n",
    "B1 = edf.Param()\n",
    "\n",
    "K2 = edf.Param()\n",
    "B2 = edf.Param()\n",
    "\n",
    "W3 = edf.Param()\n",
    "B3 = edf.Param()\n",
    "\n",
    "\n",
    "# Model\n",
    "y = edf.conv2(inp,K1)\n",
    "# y = edf.down2(y);\n",
    "y = edf.avg_pool_with_stride(y, 2, 2) # replace downsampling with avg pooling with stride\n",
    "y = edf.add(y,B1)\n",
    "y = edf.RELU(y)\n",
    "\n",
    "y = edf.conv2(y,K2)\n",
    "# y = edf.down2(y);\n",
    "y = edf.avg_pool_with_stride(y, 2, 2) # replace downsampling with avg pooling with stride\n",
    "y = edf.add(y,B2)\n",
    "y = edf.RELU(y)\n",
    "\n",
    "\n",
    "y = edf.flatten(y)\n",
    "\n",
    "y = edf.matmul(y,W3)\n",
    "y = edf.add(y,B3) # This is our final prediction\n",
    "\n",
    "\n",
    "# Cross Entropy of Soft-max\n",
    "loss = edf.smaxloss(y,lab)\n",
    "loss = edf.mean(loss)\n",
    "\n",
    "# Accuracy\n",
    "acc = edf.accuracy(y,lab)\n",
    "acc = edf.mean(acc)\n",
    "\n",
    "###################################\n",
    "\n",
    "# Init Weights\n",
    "def xavier(shape):\n",
    "    sq = np.sqrt(3.0/np.prod(shape[:-1]))\n",
    "    return np.random.uniform(-sq,sq,shape)\n",
    "\n",
    "C1 = 8\n",
    "C2 = 16\n",
    "\n",
    "K1.set(xavier((4,4,1,C1)))\n",
    "B1.set(np.zeros((C1)))\n",
    "\n",
    "K2.set(xavier((2,2,C1,C2)))\n",
    "B2.set(np.zeros((C2)))\n",
    "\n",
    "W3.set(xavier((C2*25,10)))\n",
    "B3.set(np.zeros((10)))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "BSZ=50\n",
    "lr=0.001\n",
    "\n",
    "NUM_EPOCH=10\n",
    "DISPITER=50\n",
    "batches = range(0,len(train_lb)-BSZ+1,BSZ)\n",
    "\n",
    "## Implement Momentum and uncomment following line\n",
    "edf.init_momentum()\n",
    "\n",
    "\n",
    "niter=0; avg_loss = 0.; avg_acc = 0.\n",
    "for ep in range(NUM_EPOCH+1):\n",
    "\n",
    "    # As we train, let's keep track of val accuracy\n",
    "    vacc = 0.; vloss = 0.; viter = 0\n",
    "    for b in range(0,len(val_lb)-BSZ+1,BSZ):\n",
    "        inp.set(val_im[b:b+BSZ,...]); lab.set(val_lb[b:b+BSZ])\n",
    "        edf.Forward()\n",
    "        viter = viter + 1;vacc = vacc + acc.top;vloss = vloss + loss.top\n",
    "    vloss = vloss / viter; vacc = vacc / viter * 100\n",
    "    print(\"%09d: #### %d Epochs: Val Loss = %.3e, Accuracy = %.2f%%\" % (niter,ep,vloss,vacc))\n",
    "    if ep == NUM_EPOCH:\n",
    "        break\n",
    "\n",
    "    # Shuffle Training Set\n",
    "    idx = np.random.permutation(len(train_lb))\n",
    "\n",
    "    # Train one epoch\n",
    "    for b in batches:\n",
    "        # Load a batch\n",
    "        inp.set(train_im[idx[b:b+BSZ],...])\n",
    "        lab.set(train_lb[idx[b:b+BSZ]])\n",
    "\n",
    "        edf.Forward()\n",
    "        avg_loss = avg_loss + loss.top; avg_acc = avg_acc + acc.top;\n",
    "        niter = niter + 1\n",
    "        if niter % DISPITER == 0:\n",
    "            avg_loss = avg_loss / DISPITER; avg_acc = avg_acc / DISPITER * 100\n",
    "            print(\"%09d: Training Loss = %.3e, Accuracy = %.2f%%\" % (niter,avg_loss,avg_acc))\n",
    "            avg_loss = 0.; avg_acc = 0.;\n",
    "\n",
    "        edf.Backward(loss)\n",
    "        #edf.SGD(lr)\n",
    "        # Replace previous line with following\n",
    "        edf.momentum(lr,0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intstatnn]",
   "language": "python",
   "name": "conda-env-intstatnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
